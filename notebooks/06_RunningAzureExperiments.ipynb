{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-01 15:57:06,175 - azureml.core.workspace - INFO - Found the config file in: /home/yuvraj/projects/AzureAworkspace/azureds/.azureml/config.json\n",
      "Ready to use Azure ML 1.2.0 to work with azml-workspace\n",
      "Imported workspace as ws\n",
      "2020-04-01 15:57:08,281 - root - INFO - Defined global variable `ws` and `conf_catalog`\n"
     ]
    }
   ],
   "source": [
    "%reload_azureml_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Experiment \n",
    "\n",
    "\n",
    "## 1.1 Azure Experiment \n",
    "\n",
    "In AzureML, an experiment is an abstraction model that let you run a script or a pipeline. The main features of an experiment is the ability to generate metrics and outputs that can be tracked in the Azure Machine Learning Workspace.\n",
    "\n",
    "When you submit an experiment, you use its run context to initialize and end the experiment run that is tracked in Azure Machine Learning, as shown in the following code sample:\n",
    "\n",
    ">```python\n",
    "from azureml.core import Experiment\n",
    "# create an experiment variable\n",
    "experiment = Experiment(workspace = ws, name = \"my-experiment\")\n",
    "# start the experiment\n",
    "run = experiment.start_logging()\n",
    "# experiment code goes here\n",
    "def do_something():\n",
    "    return pass\n",
    "# end the experiment\n",
    "run.complete()\n",
    "```\n",
    "\n",
    "## 1.2 Experiment Logging Metric and Outputs\n",
    "\n",
    "Every experiment generates log files that include the messages that would be written to the terminal during interactive execution. This enables you to use simple print statements to write messages to the log. However, if you want to record named metrics for comparison across runs, you can do so by using the Run object; which provides a range of logging functions specifically for this purpose. These include:\n",
    "\n",
    "- ***log:*** Record a single named value.\n",
    "- ***log_list:*** Record a named list of values.\n",
    "- ***log_row:*** Record a row with multiple columns.\n",
    "- ***log_table:*** Record a dictionary as a table.\n",
    "- ***log_image:*** Record an image file or a plot.\n",
    "\n",
    "For example, following code records the number of observations (records) in a CSV file:\n",
    "\n",
    ">```python\n",
    "from azureml.core import Experiment\n",
    "import pandas as pd\n",
    "# Create an Azure ML experiment in your workspace\n",
    "experiment = Experiment(workspace = ws, name = 'my-experiment')\n",
    "# Start logging data from the experiment\n",
    "run = experiment.start_logging()\n",
    "# load the dataset and count the rows\n",
    "data = pd.read_csv('data.csv')\n",
    "row_count = (len(data))\n",
    "# Log the row count\n",
    "run.log('observations', row_count)\n",
    "# Complete the experiment\n",
    "run.complete()\n",
    "```\n",
    "\n",
    "## 1.3 Running an Experiment Script\n",
    "To run a script as an experiment, you must define a run configuration that defines the Python environment in which the script will be run, and a script run configuration that associates the run environment with the script. These are implemented by using the RunConfiguration and ScriptRunConfig objects.\n",
    "\n",
    "For example, the following code could be used to run an experiment based on a script in the experiment_files folder (which must also contain any files used by the script, such as the data.csv file in previous script code example):\n",
    "\n",
    "```python\n",
    "from azureml.core import Experiment, RunConfiguration, ScriptRunConfig\n",
    "\n",
    "# create a new RunConfig object\n",
    "experiment_run_config = RunConfiguration()\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder, \n",
    "                      script='experiment.py',\n",
    "                      run_config=experiment_run_config) \n",
    "\n",
    "# submit the experiment\n",
    "experiment = Experiment(workspace = ws, name = 'my-experiment')\n",
    "run = experiment.submit(config=script_config)\n",
    "run.wait_for_completion(show_output=True)\n",
    "```\n",
    "\n",
    ">Note: The RunConfig object defines the Python environment for the experiment, including the packages available to the script. If your script depends on packages that are not included in the default environment, you must associate the RunConfig with an Environment object that makes use of a CondaDependencies object to specify the Python packages required. Runtime environments are discussed in more detail later in this course.\n",
    "\n",
    "# 1.4 View Experiment Results\n",
    "\n",
    "After the experiment has been finished, you can use the **run** object to get information about the run and its outputs:\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "# Get run details\n",
    "details = run.get_details()\n",
    "print(details)\n",
    "\n",
    "# Get logged metrics\n",
    "metrics = run.get_metrics()\n",
    "print(json.dumps(metrics, indent=2))\n",
    "\n",
    "# Get output files\n",
    "files = run.get_file_names()\n",
    "print(json.dumps(files, indent=2))\n",
    "```\n",
    "\n",
    "In Jupyter Notebooks, you can use the **RunDetails** widget to get a better visualization of the run details.\n",
    "\n",
    "```python\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()\n",
    "```\n",
    "\n",
    "Note that the **RunDetails** widget includes a link to view the run in Azure Machine Learning studio. Click this to open a new browser tab with the run details (you can also just open [Azure Machine Learning studio](https://ml.azure.com) and find the run on the **Experiments** page). When viewing the run in Azure Machine Learning studio, note the following:\n",
    "\n",
    "- The **Properties** tab contains the general properties of the experiment run.\n",
    "- The **Metrics** tab enables you to select logged metrics and view them as tables or charts.\n",
    "- The **Images** tab enables you to select and view any images or plots that were logged in the experiment (in this case, the *Label Distribution* plot)\n",
    "- The **Child Runs** tab lists any child runs (in this experiment there are none).\n",
    "- The **Outputs** tab shows the output files generated by the experiment.\n",
    "- The **Logs** tab shows any logs that were generated by the compute context for the experiment (in this case, the experiment was run inline so there are no logs).\n",
    "- The **Snapshots** tab contains all files in the folder where the experiment code was run (in this case, everything in the same folder as this notebook).\n",
    "- The **Raw JSON** tab shows a JSON representation of the experiment details.\n",
    "- The **Explanations** tab is used to show model explanations generated by the experiment (in this case, there are none).\n",
    "\n",
    "# 1.5 Running Experiment Script\n",
    "\n",
    "To run a script as an experiment, you must define a run configuration that defines the Python environment in which the script will be run, and a script run configuration that associates the run environment with the script. These are implemented by using the ***RunConfiguration*** and ***ScriptRunConfig*** objects.\n",
    "\n",
    "For example, the following code could be used to run an experiment based on a script in the experiment_files folder (which must also contain any files used by the script, such as the data.csv file in previous script code example):\n",
    "\n",
    "``` Python\n",
    "from azureml.core import Experiment, RunConfiguration, ScriptRunConfig\n",
    "\n",
    "# create a new RunConfig object\n",
    "experiment_run_config = RunConfiguration()\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder, \n",
    "                      script='experiment.py',\n",
    "                      run_config=experiment_run_config) \n",
    "\n",
    "# submit the experiment\n",
    "experiment = Experiment(workspace = ws, name = 'my-experiment')\n",
    "run = experiment.submit(config=script_config)\n",
    "run.wait_for_completion(show_output=True)\n",
    "```\n",
    "\n",
    "Note: The RunConfig object defines the Python environment for the experiment, including the packages available to the script. If your script depends on packages that are not included in the default environment, you must associate the RunConfig with an Environment object that makes use of a CondaDependencies object to specify the Python packages required. Runtime environments are discussed in more detail later in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Azure Estimators\n",
    "\n",
    "You can use a ***Run Configuration*** and a ***Script Run Configuration*** to run a script-based experiment that trains a machine learning model. However, depending on the machine learning framework being used and the dependencies it requires, the run configuration may become complex. \n",
    "\n",
    "Azure Machine Learning provides a higher level abstraction called an ***Estimator*** that encapsulates a run configuration and a script configuration in a single object, and for which there are pre-defined, framework-specific variants that already include the package dependencies for common machine learning frameworks such as Scikit-Learn, PyTorch, and Tensorflow.\n",
    "\n",
    "##  2.1 Writing a script for an estimator\n",
    "\n",
    "When using an experiment to train a model, your script should save the trained model in the outputs folder. For example, the following code shows how a model trained using Scikit-Learn can be saved in the outputs folder using the joblib package:\n",
    "\n",
    "\n",
    "```python\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Prepare the dataset\n",
    "diabetes = pd.read_csv('data.csv')\n",
    "X, y = data[['Feature1','Feature2','Feature3']].values, data['Label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "# Train a logistic regression model\n",
    "reg = 0.1\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/model.pkl')\n",
    "\n",
    "run.complete()\n",
    "\n",
    "```\n",
    "\n",
    "## 2.2 Using an Estimator\n",
    "\n",
    "You can use a generic Estimator class to define a run configuration for a training script like this:\n",
    "\n",
    "```python\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Create an estimator\n",
    "script_params = {\n",
    "   '--num_epochs': 20,\n",
    "   '--data_dir': ds_data.as_mount(),\n",
    "   '--output_dir': './outputs'\n",
    "}\n",
    "\n",
    "estimator = Estimator(source_directory=project_folder,\n",
    "                     compute_target=compute_target,\n",
    "                     entry_script='cntk_distr_mnist.py',\n",
    "                     script_params=script_params,\n",
    "                     node_count=2,\n",
    "                     process_count_per_node=1,\n",
    "                     distributed_backend='mpi',\n",
    "                     pip_packages=['cntk-gpu==2.6'],\n",
    "                     custom_docker_image='microsoft/mmlspark:gpu-0.12',\n",
    "                     use_gpu=True)\n",
    "                     \n",
    "# Create and run an experiment\n",
    "experiment = Experiment(workspace = ws, name = 'training_experiment')\n",
    "run = experiment.submit(config=estimator)\n",
    "```\n",
    "\n",
    ">NOTE: See the following [link](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/distributed-cntk-with-custom-docker/distributed-cntk-with-custom-docker.ipynb) for a more comprehensive example of using estimators with azure cognitice services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AzureDataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
