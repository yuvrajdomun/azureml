{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_azureml_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Azure Datasets\n",
    "\n",
    "An Azure [***Dataset***](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py) represents a resource for \n",
    "\n",
    "- exploring,\n",
    "- transforming, and\n",
    "- managing \n",
    "\n",
    "data in AzureML.\n",
    "\n",
    "Datasets are the recommended way to work with data, and are the primary mechanism for advanced Azure Machine Learning capabilities like data labeling and data drift monitoring.\n",
    "\n",
    ">*** A Dataset is a reference to data in a Datastore or behind public web urls.***\n",
    "\n",
    "## 1.1 Type of Azure Datasets \n",
    "\n",
    "You can create the following types of dataset:\n",
    "\n",
    "- ***Tabular*** : The data is read from the dataset as a table. You should use this type of dataset when your data is consistently structured and you want to work with it in common tabular data structures, such as Pandas dataframes.\n",
    "\n",
    "- ***File*** : The dataset presents a list of file paths that can be read as though from the file system. Use this type of dataset when your data is unstructured, or when you need to process the data at the file level (for example, to train a convolutional neural network from a set of image files)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating and Registering Tabular Datasets\n",
    "\n",
    "## 2.1 Create Tabular Dataset\n",
    "To create a tabular dataset using the SDK, use the from_delimited_files method of the Dataset.Tabular class, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "#Create a tabular dataset from the path on the datastore (this may take a short while)\n",
    "tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds,'01_raw/*.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Register Tabular Dataset\n",
    "We will register the tabular dataset as Iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the tabular dataset\n",
    "tab_data_set = tab_data_set.register(workspace=ws, \n",
    "                                   name='Iris Dataset',\n",
    "                                   description='iris data',\n",
    "                                   tags = {'format':'CSV'},\n",
    "                                   create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Creating and Registering File Datasets\n",
    "\n",
    "To create a file dataset using the SDK, use the from_files method of the Dataset.File class, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a file dataset from the path on the datastore (this may take a short while)\n",
    "file_data_set = Dataset.File.from_files(path=(default_ds, '01_raw/*.csv'))\n",
    "\n",
    "# Get the files in the dataset\n",
    "for file_path in file_data_set.to_path():\n",
    "    print(file_path)\n",
    "\n",
    "try:\n",
    "    # Register the file dataset\n",
    "    file_data_set = file_data_set.register(workspace=ws, \n",
    "                                       name='Iris Files Dataset',\n",
    "                                       description='Iris file dataset',\n",
    "                                       tags = {'format':'CSV'},\n",
    "                                       create_new_version=True)\n",
    "    print('Datasets registered')\n",
    "except:\n",
    "    print('Dataset File already registered')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view and manage datasets on the Datasets page for your workspace in Azure ML Studio. You cal also get a list of datasets from the workspace object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Datasets:\")\n",
    "for dataset_name in list(ws.datasets.keys()):\n",
    "    dataset = Dataset.get_by_name(ws, dataset_name)\n",
    "    print(\"\\t\", dataset.name, 'version', dataset.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Retrieving a Registered Dataset\n",
    "After registering a dataset, you can retrieve it by using any of the following techniques:\n",
    "\n",
    "- The ***datasets*** dictionary attribute of a ***Workspace*** object.\n",
    "- The ***get_by_name*** or ***get_by_id*** method of the ***Dataset*** class.\n",
    "\n",
    "Both of these techniques are shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "# Get a dataset from the workspace datasets collection\n",
    "ds1 = ws.datasets['Iris Dataset']\n",
    "\n",
    "# Get a dataset by name from the datasets class\n",
    "ds2 = Dataset.get_by_name(ws, 'Iris Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 20 rows as a Pandas dataframe\n",
    "\n",
    "data = ds1.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ws.datasets.get('Iris Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AzureDataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
